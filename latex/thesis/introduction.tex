\chapter{Introduction}
Neural Networks have achieved remarkable success in various tasks, however, their increasingly large size leads to high computational and memory requirements.
Neural Network Pruning \autocite{LeCun, OptimalBrainSurgeon, HanEtAl15, PruningFiltersForEfficientConvets} has been proposed to reduce the number of parameters after the network is trained while maintaining accuracy.
This leads to a size reduction \autocite{HanEtAl15} and energy consumption \autocite{YangCS17} in the resulting subnetworks during inference.

To increase efficiency during training as well, the smaller networks need to be trained from the beginning. Attempts to train smaller networks discovered by pruning from scratch did not yield convincing results \autocite{HanEtAl15, PruningFiltersForEfficientConvets}.

The Lottery Ticket Hypothesis by \textcite{LTH} has shown that there exist small, well-initialized subnetworks that can be trained in isolation to reach the same accuracy as the unpruned networks. 
In contrast to previous attempts where the subnetwork structure after pruning is randomly reinitialized \autocite{HanEtAl15, PruningFiltersForEfficientConvets}, \textcite{LTH} reinitialize the subnetworks with their respective initial weights from before training.
The subnetwork structure is uncovered by an algorithm called Iterative Magnitude Pruning (IMP).
Different approaches for discovering small, well-performing subnetworks were developed \autocite{Supermasks}, Edge Popup \autocite{EdgePopup}, and Gem-Miner\autocite{RareGems}.

However, the structure of the underlying sparse subnetwork is rarely discussed.
Since the networks are sparse, often to a high degree, the structure might give valuable insights into the functionality of the subnetwork.
Yet, analyzing the structure of the network is not an easy undertaking.

\section{Motivation}
The study of lottery tickets itself has broad ramifications.
At its ultimate, it could reduce the size of the neural networks significantly during training.
This would result in lower computational and memory requirements and therefore cost and energy savings.
It could also enable us to train models larger than previously possible, due to an increased efficiency of each parameter.
Studying the structure of lottery tickets might produce valuable insights that further advance the field.

Further, since the lottery tickets are highly sparse, they represent a very reduced function that solves the problem.
This might turn out valuable for network interpretation.
The structure of the network might yield information about how the network solves a task.
This could enrich the current ecosystem of techniques to interpret neural networks.

\section{Objective}
The objective of this thesis is to attain knowledge about the structure of lottery ticket subnetworks.
Since the structure of a sparse neural network is a directed acyclic graph with larger numbers of nodes and edges, analyzing such a structure is difficult.
Therefore, the problem is approached from the other direction.
One could take a \textit{dataset} with a known structure and test if the resulting sparse networks resemble that structure in any way.
For instance, a problem that consists of two independent subproblems may result in two independent subnetworks, one for each task. 
Within this thesis, this question is empirically studied.

\section{Research Question}
The research question that follows from the objective is:

\begin{quote}
    Does the structure of a lottery ticket subnetwork exhibit features of the semantic structure of the data it was trained on?
\end{quote}

This question cannot be fully answered within the scope of this thesis, therefore a more detailed question is formulated:

\begin{quote}
    Does a lottery ticket that was trained on a dataset with two independent tasks contain two independent subnetworks?
 \end{quote}
 Since the network is still considered a lottery ticket, this implies that it has to maintain its performance at the time it separates into two.


\section{Structure of the Thesis}
In the first chapter, a literature review is conducted, where relevant topics to this thesis are described.
The next chapter explains the methodology.
The dataset, neural network architecture, and training process are outlined, as well as other algorithms used in the experiments.
The following chapter walks through the experiments that were conducted during this thesis.
It contains the experiment setup, the results the interpretation of those.
The thesis is concluded with a discussion of future research in this direction.