\chapter{Introduction}
Neural Networks have achieved remarkable success in various tasks, however, their increasingly large size leads to high computational and memory requirements.
Neural Network Pruning \autocite{LeCun, OptimalBrainSurgeon, HanEtAl15, PruningFiltersForEfficientConvets} has been proposed to reduce the number of parameters after the network is trained while maintaining accuracy.
This leads to a size reduction \autocite{HanEtAl15} and energy consumption \autocite{YangCS17} in the resulting subnetworks during inference.
To increase efficiency during training as well, the smaller networks need to be trained from the beginning. Attempts to train smaller networks discovered by pruning from scratch did not yield convincing results \autocite{HanEtAl15, PruningFiltersForEfficientConvets}.

The Lottery Ticket Hypothesis by \textcite{LTH} has shown that there exist small, well-initialized subnetworks that can be trained in isolation to reach the same accuracy as the unpruned networks. 
In contrast to previous attempts where the subnetwork structure after pruning is randomly reinitialized \autocite{HanEtAl15, PruningFiltersForEfficientConvets}, \textcite{LTH} reinitialize the subnetworks with their respective initial weights from before training.
The subnetwork structure is uncovered by an algorithm called Iterative Magnitude Pruning (IMP).
The reason why these lottery ticket networks perform so well, or why they can be trained in isolation remains elusive.
Since the lottery ticket networks are sparse, their structure might give valuable insights into their functionality.

\section{Motivation}
The study of lottery tickets itself has broad ramifications.
At its ultimate, it could reduce the size of the neural networks significantly during training.
This would result in lower computational and memory requirements and therefore cost and energy savings.
It could also enable us to train models larger than previously possible, due to an increased efficiency of each parameter.
Studying the structure of lottery tickets might produce valuable insights that further advance the field.

Further, since the lottery tickets are highly sparse, they represent a very reduced function that solves the problem.
This might turn out valuable for network interpretation.
The structure of the network might yield information about how the network solves a task.
This could enrich the current ecosystem of techniques to interpret neural networks.

\section{Objective}
The objective of this thesis is to attain knowledge about the structure of lottery ticket subnetworks.
Since the structure of a lottery ticket, which is a sparse neural network, is a directed acyclic graph with larger numbers of nodes and edges, analyzing such a structure is difficult.
Therefore, the problem is approached from a different direction.
One could take a dataset with a known structure and test if the resulting sparse networks resemble that structure in any way.
For instance, a problem that consists of two independent subproblems may result in a lottery ticket containing two independent subnetworks, one for each task. 
Within this thesis, this question is empirically studied.

\section{Research Question}
A general research question that follows from the objective is: \textit{``Does the structure of a lottery ticket subnetwork exhibit features of the semantic structure of the data it was trained on?''}
This question defines the overarching idea pursued in this thesis.
A more concrete and restricted question, which could be considered a subquestion of the one previously stated:

\begin{quote}
\textit{``Does a lottery ticket that was trained on a dataset with two independent tasks contain two independent subnetworks?''}
\end{quote}

The experiments in this thesis aim to answer this very question.

\section{Structure of the Thesis}
In the first chapter, a literature review is conducted, where topics relevant to this thesis are described.
The next chapter explains the methodology.
The dataset, neural network architecture, and training process are outlined, as well as other algorithms used in the experiments.
The following chapter walks through the experiments that were conducted during this thesis.
Further, the results are discussed and interpreted.
The thesis is concluded with a discussion and possible future research in this direction.