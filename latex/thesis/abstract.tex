\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

The \textbf{Lottery Ticket Hypothesis} by \textcite{LTH} has sparked a novel field of research that concerns itself with well-trainable sparse neural networks.
One key method in this field is called \textit{Iterative Magnitude Pruning}, which is successful in finding sparse subnetworks that can be trained to high performance. These networks are called \textit{Winning Tickets}.
Many aspects of why and how this procedure works and what characteristics of the winning tickets make them successful remain elusive.
One way to learn more about lottery tickets would be to study their network structure.
Since they are usually highly sparse, their structure may contain valuable insights into their functionality.
In this thesis, the structure of winning ticket networks is analyzed and how it relates to the data the network was trained on.
The experiments in this thesis are conducted on datasets that contain two independent tasks \rule[0.5ex]{.5em}{0.5pt} a simple toy dataset and a combination of the MNIST and Fashion-MNIST datasets.
On both datasets, winning tickets derived with iterative magnitude pruning are found that contain separate, independent subnetworks where each subnetwork solves one task.
