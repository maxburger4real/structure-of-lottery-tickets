\begin{abstract}
\noindent Pruning neural networks is a successful technique to reduce parameter count whilst maintaining accuracy of neural networks. 
Training the pruned subnetworks from scratch however, does not result in compelling accuracy. 
(\cite{DBLP:conf/iclr/FrankleC19}) show that the initialization makes all the difference and that they can indeed train to full accuracy. Based on this insight, they state the lottery ticket hypothesis.
Well performing subnetworks were found in different scenarios with Iterative Magnitude Pruning and resetting the resulting sparse network with the weights at initialization. These subnetworks are called winning tickets. Alternative methods for finding well-performing subnetworks without any weight training (\cite{DBLP:conf/cvpr/RamanujanWKFR20}), or with rewinding the weights to an early state of training (\cite{LinearModeConnectivity}) are described. These methods are the basis for a recent method called Gem-Miner (\cite{RareGems}), which finds well-performing subnetworks in a more efficient manner. 
\end{abstract}