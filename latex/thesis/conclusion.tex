\chapter{Conclusion}


\section{Seminar Conclusion}
Neural Networks have achieved remarkable success in various tasks, however, their increasingly large size leads to high computational and memory requirements.
Neural Network Pruning \autocite{LeCun, OptimalBrainSurgeon, HanEtAl15, PruningFiltersForEfficientConvets} has been proposed to reduce the number of parameters after the network is trained, while maintaining accuracy.
This leads to a reduction in size \autocite{HanEtAl15} and energy consumption \autocite{YangCS17} in the resulting subnetworks during inference.
To increase efficiency during training as well, the smaller networks need to be trained from the beginning. Attempts to train smaller networks discovered by pruning from scratch did not yield convincing results \autocite{HanEtAl15, PruningFiltersForEfficientConvets}.
The Lottery Ticket Hypothesis by \textcite{DBLP:conf/iclr/FrankleC19} has shown that there exist small, well-initialized subnetworks that can be trained in isolation to reach the same accuracy as the unpruned networks. 
In contrast to previous attempts where the subnetwork structure after pruning is randomly reinitialized \autocite{HanEtAl15, PruningFiltersForEfficientConvets}, \textcite{DBLP:conf/iclr/FrankleC19} reinitialize the subnetworks with their respective initial weights from before training.
The subnetwork structure is uncovered by an algorithm called Iterative Magnitude Pruning (IMP).
This paper surveys the recent advancements in finding well-performing subnetworks and the characteristics of these networks. Different approaches for discovering small, well-performing subnetworks are presented, including Supermasks \autocite{DBLP:conf/nips/ZhouLLY19}, Edge Popup\autocite{DBLP:conf/cvpr/RamanujanWKFR20}, and Gem-Miner\autocite{RareGems}.