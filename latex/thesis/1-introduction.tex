\chapter{Introduction}
Neural network pruning techniques are successful in significantly reducing the number of network parameters without compromising the performance~\autocite{OptimalBrainDamage, OptimalBrainSurgeon}.  
Attempts to train the pruned models after they have been randomly reinitialized did not yield satisfying results~\autocite{HanEtAl15, PruningFiltersForEfficientConvets}.
The Lottery Ticket Hypothesis by \textcite{LTH} has shown that there exist small, well-initialized subnetworks that can be trained in isolation to reach the same accuracy as the original dense networks.
These small subnetworks, the so-called \textit{Winning Tickets}, are created by iterative training and pruning of a fully connected dense network.
This process is called \textit{Iterative Magnitude Pruning}.
The work by \textcite{LTH} defines a promising avenue towards the ability to train smaller sparse networks from the beginning.
However, several aspects of why winning tickets can learn so well and why iterative magnitude pruning can uncover them are not yet fully understood.

Although there is plenty of research in this field, the structure of the winning tickets is rarely discussed.
Since the winning ticket networks are sparse, their structure might give valuable insights into their functionality.
In this thesis, the structure of winning tickets and how it relates to the function of the network is studied.

\section{Motivation}
Understanding more about the structure of winning tickets can lead to numerous advances.
By studying the structure, insights about the algorithms like iterative magnitude pruning, that are used to uncover the winning tickets, can be generated.
Potential drawbacks of the algorithm can be discovered by analyzing weaknesses in the network structure.
Further, the structure could give insights into the function the network has learned.
Due to the high sparsity of winning tickets, the network structure might reflect the higher-level semantic structure of the learned function.
This in turn can be instrumental for network interpretation.
Winning tickets could be used to find substructures in the functions they learn, which in turn could improve the interpretability.
The objective of this thesis is to find out more about the structure of winning tickets and whether they contain meaningful features.

\section{Research Questions and Scope}
The general research question that follows from the objective is: 
\begin{quote}
\textit{Do winning tickets contain meaningful structures?}
\end{quote}
This question serves as the main research direction.
Due to the generality of the question, it is narrowed down for this thesis to be approachable.
Only standard fully connected feed-forward neural networks are used to derive winning tickets.
To uncover the winning tickets, iterative magnitude pruning is employed.
This results in a narrower research question:
\begin{quote}
\textit{Do winning tickets of fully connected feed-forward networks derived with iterative magnitude pruning contain meaningful structures?}
\end{quote}
This thesis contains empirical research that aims to answer this very question.


\section{Structure of the Thesis}
In chapter~\ref{literature_review}, a literature review is conducted, where topics relevant to this thesis are described.
Chapter~\ref{chapter:method} explains the methodology.
The neural network architecture and training process are outlined, as well as other algorithms and techniques used in the experiments.
Chapter~\ref{chapter:experiments} walks through the creation of the datasets as well as the experiments that were conducted during this thesis.
The experiment results are discussed and interpreted.
The thesis is concluded with a discussion and possible future research in this direction in chapter~\ref{chapter:conclusion}.