\chapter{Literature Review}

overparameterization: [3] Yann Dauphin and Yoshua Bengio. Big neural networks waste capacity. CoRR, abs/1301.3583, 2013.

Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148-2156, 2013.

Great intro to compression in Zhou et al Supermasks.

TODO introduction to the literature review  
at the core this thesis is about understanding the structure of lottery tickets. So what is or was important about that?
\begin{itemize}
    \item lottery ticket Hypothesis, its relevance 
    \item derivative works of the lth, attempts to understand it
    \item structure of lottery tickets? is there really nothing? connectedness, degree (original LTH), Han et al says there will be dead neurons, All allive pruning says they kill them
    \item understanding networks, mechanistic interpretability, independence as a nice thing to understand stuff. Bimt showing independece is in the network
\end{itemize}

Is the structure of the lottery ticket useful? interpretable? or is it merely super overfitted random shit that doesnt help you anyway.

\section{Deep Learning}
The field of deep learning is largely responsible for the advances of modern machine learning applications.
With increasing amounts of available data and computational resources, the size of neural networks has increased drastically, as well as their performance.
However, this increase in size poses challenges in constrained environements.
Larger and deeper networks tend to require more memory and compute, which can be problematic for inference on constrained devices.
Also, larger models typically require more time and more energy for training as well as for inference.
Neural Network Pruning \autocite{LeCun, OptimalBrainSurgeon, HanEtAl15, PruningFiltersForEfficientConvets} has been proposed as a tool to reduce network size for inference, while maintaining accuracy.

\section{Neural Network Pruning}
Neural Network Pruning is a category of methods that eliminate components of the network whilst aiming to minimally impact its performance.
Pruning algorithms can reduce parameter count by up to 90\% without harming accuracy.
\autocite{LeCun, OptimalBrainSurgeon, HanEtAl15, PruningFiltersForEfficientConvets}
Pruning can be done in a single step, where the network is trained once and then pruned.
It can also be done iteratively. 
The network is repeatedly trained and pruned over several iterations. 
It has been shown that iterative pruning can further reduce network size significantly whilst maintaining performance \autocite{HanEtAl15}.
The resulting sparse subnetwork can represent an equally accurate function with significantly fewer parameters. 
The question arises: If it can represent the function, why not train the sparse network directly?
An examination into training pruned networks from the beginning was performed by \textcite{PruningFiltersForEfficientConvets} and \textcite{HanEtAl15}.
The subnetwork is randomly reinitialized and trained in isolation.
With this approach, the subnetworks did not reach comparable accuracy to the unpruned networks.
These findings seemingly demonstrate the difficulty of training sparse neural networks.
An investigation into this apparent shortcoming of sparse networks was conducted by \textcite{DBLP:conf/iclr/FrankleC19}.
The authors discover the phenomenon that the sparse neural networks they created can indeed train to comparable accuracy.
The condition for this to work, is that they are not randomly reinitialized. 
Instead, the respective values at initialization of the unpruned network are assigned to the remaining weights of the pruned network. 
Based on this finding, the authors formulate the \textit{Lottery Ticket Hypothesis}.

\section{The Lottery Ticket Hypothesis}
The basis of the field of Lottery Tickets is the work of~\cite{DBLP:conf/iclr/FrankleC19}. 
They show that dense neural networks consistently contain subnetworks that can be trained in isolation to comparable accuracy.
They find these well-performing subnetworks, which they call \textit{winning tickets}, in a variety of scenarios.
They are found with fully connected neural networks (FCNN) on MNIST\footnote{\cite{mnist}} and with convolutional neural networks\footnote{\cite{cnn}} (CNN)  on CIFAR-10\footnote{\cite{cifar}}.
Further, they are found with different optimizers (ADAM\footnote{\cite{ADAM}}, Stochastic Gradient Descent - SGD, SGD with Momentum) and different regularization techniques.
Based on these findings, the authors propose:

\begin{quote}
    “\textbf{The Lottery Ticket Hypothesis}: \textit{A randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.}” \cite{DBLP:conf/iclr/FrankleC19}
\end{quote}

To identify the winning tickets, a network is trained to convergence and pruned to a desired sparsity. 
The bottom $p$\% of weights in each layer with the lowest magnitudes are removed. 
The remaining weights of the network are reset to the respective values of the weights at initialization.
The result is a sparse subnetwork, a \textit{winning ticket}.

More formally, consider a neural network $f(x, \theta)$ with randomly initialized weights $\theta$ and the network input $x$.
This network is trained to convergence, producing the updated weights $\hat \theta$. The test accuracy of the resulting network is denoted $\hat a$.
Then a pruning function is used to create a mask $m \in \{0,1\}^{|\theta|}$, where $|\theta|$ denotes the number of parameters in $\theta$.
This mask is multiplied element-wise with the initial weights, producing the subnetwork weights denoted $\theta_{wt} = m \odot \theta$, where $\odot$ denotes the element-wise product.
A neural network is initialized with the weights $\theta_{wt}$ and trained to convergence, resulting in the trained weights $\hat \theta_{wt}$. 
The test accuracy of the resulting trained subnetwork is denoted $\hat a_{wt}$.

The subnetwork is considered a winning ticket, if:
\begin{enumerate}
  \item  $\hat a_{wt} \geq \hat a$ :  the accuracy of the trained subnetwork is larger than or equal to the accuracy of the  trained dense network.
  \item $|\theta_{wt}| \ll |\theta|$ : the subnetwork has significantly fewer parameters than the dense network.
\end{enumerate}

The authors use an iterative approach to identify winning tickets, called Iterative Magnitude Pruning (IMP).
They show that the iterative approach results in better performing winning tickets compared to so called \textit{One-Shot} pruning, where the network is trained to convergence once and then pruned once.

Concretely, consider a function  $\textit{P} : \mathbb{R}^{|\theta|} \to \{0,1\}^{|\theta|}$.
The output of the function is a pruning mask for a given set of parameters.
It masks out the $p^{\frac{1}{N}}$\% of unmasked weights in each layer with the lowest magnitudes, where $p$ is the desired final pruning percentage and $N$ the number of iterations of IMP.

Now, a neural network with initial weights $\theta^{(0)}$ is trained to convergence, resulting in the trained parameters $\hat \theta^{(0)}$.
Then, the function $\textit{P}$ is applied to the trained parameters, resulting in the mask $m^{(0)} = \textit{P}(\hat \theta^{(0)})$.
The mask is multiplied element-wise with the initial weights, producing the weights for the next iteration $\theta^{(1)} = m^{(0)} \odot \theta^{(0)}$.
These weights are used to initialize the network, which is then trained to convergence resulting in $\hat \theta^{(1)}$ and the whole process repeats.

This procedure is repeated $N$-times, resulting in the update rules:

$$
m^{(i)} = \textit{P}(\hat \theta^{(i)}); \quad i \in (0,N-1)
$$

$$
\theta^{(i+1)} = \theta^{(0)} \odot \prod_{j=0}^{i}m^{(j)}; \quad i \in (0,N-1)
$$ 

where $\prod$ denotes the element-wise product and $\hat \theta^{(i)}$ denotes the updated weights after training a network with weights $\theta^{(i)}$.

Because all masks are multiplied with the initial weights, the resulting winning ticket subnetwork can be expressed as:

$$
\theta_{wt} = \theta^{(N)} = \theta^{(0)} \odot \prod_{j=0}^{N-1}m^{(j)}
$$
In the final winning wicket weights, $p$\% of the weights are masked out.

The experiments of \autocite{DBLP:conf/iclr/FrankleC19} show that IMP as described above suffices for finding winning tickets in small architectures.
Concretely, for LeNet\footnote{\cite{cnn}} and CNNs that are smaller variants of VGG\footnote{\cite{SimonyanZisserman}}.
Experiments were also conducted on larger, more commonly used networks for CIFAR-10. For IMP to obtain winning tickets in VGG-19\footnote{\cite{Liu19}} and ResNet-18\footnote{\cite{ResidualConnect}}, the following adaptations were required. 
\begin{enumerate}
  \item  Pruning is no longer done layer-wise but globally, as it results in smaller winning tickets. 
  \item Learning rate warm-up is employed. The learning rate increases linearly from 0 to the specified final learning rate in the first $k$ iterations. For VGG-19 and ResNet-18, $k$ is 10000 and 20000 respectively.
\end{enumerate}

In follow-up work by \textcite{LinearModeConnectivity}, the authors address the problems that arise at scale. Experiments done by \textcite{Liu19, Gale19} show that in more challenging settings, subnetworks obtained with IMP do not perform better than randomly sampled subnetworks. To understand this phenomenon, \textcite{LinearModeConnectivity} propose a tool for understanding the failings of IMP in uncovering winning tickets. For that, the authors introduce \textit{Instability analysis}.

Instability refers to the network being susceptible to noise of Stochastic Gradient Descent (SGD).
SGD introduces noise by stochastically selecting the order of mini-batches for the network training.
To determine if a network is stable to this noise, first the network has to be duplicated.
The two identical networks are then trained with different SGD noise, realized with different random seeds.
After training, there are two sets of weights, $\theta_A$ and $\theta_B$.
Between both sets of weights, a linear path is defined. Along this path, the weights are linearly interpolated with $\theta_\alpha = \alpha \theta_A + (1 - \alpha \theta_B)$, where $\alpha \in (0,1)$. 
For each of the interpolated sets of weights $\theta_\alpha$, the performance, or error, is measured.
The highest increase in error of the interpolated weight sets $\theta_\alpha$ relative to the mean of the error of the original weights $\theta_A$ and $\theta_B$ is called the \textit{error barrier height}. 
If the error barrier height is smaller than a specified threshold, the network is considered linear mode connected and, hence, \textit{stable}.
The authors use a threshold of 2\% as the threshold in their experiments, intended to tolerate noise. The implementation of the linear interpolation between the two weight sets is realized with 30 evenly spaced values of $\alpha$.

\textcite{LinearModeConnectivity} conduct Instability Analysis on unpruned dense networks with different architectures and datasets at initialization as well as during training. For this experiment, they examined LeNet\footnote{\cite{cnn}} trained on MNIST, ResNet-20\footcite{ResidualConnect} and VGG-16\footcite{SimonyanZisserman} trained on CIFAR-10 as well as ResNet-50\footcite{ResidualConnect} and Inception-v3\footcite{inceptionv3} on ImageNet\footcite{imagenet}. They find that only LeNet is stable at initialization, however all other networks become stable early in training.
Furthermore and more interestingly, the authors conduct Instability Analysis on subnetworks uncovered by IMP.
In Addition, for this purpose the IMP-algorithm is generalized to rewind the weights to any iteration $k$ in training.

Concretely, consider a neural network with initial weights $\theta^{(0)}$. The network is trained for $k$-iterations resulting in the weights $\theta_k^{(0)}$. Then it is further trained to convergence, resulting in the trained parameters $\hat \theta^{(0)}$.
The function $P$ that generates the mask is again applied to the trained weights, resulting in the mask resulting in the mask $m^{(0)} = \textit{P}(\hat \theta^{(0)})$.

This procedure is repeated $N$-times, resulting in the update rules:

$$
m^{(i)} = \textit{P}(\hat \theta^{(i)}); \quad i \in (0,N-1)
$$


$$\theta^{(i+1)} = \theta_k^{(0)} \odot \prod_{j=0}^{i}m^{(j)}; \quad i \in (0,N-1)$$ where $\prod$ denotes the element-wise product and $\hat \theta^{(i)}$ denotes the updated weights after training a network with weights $\theta^{(i)}$.

The resulting subnetwork can be written as:
$$
\theta_{match} = \theta_{k}^{(0)}\prod_{j=1}^{N-1}m_j
$$
, where $N$ refers to the number of iterations of IMP.

The condition for winning tickets is, that the weights are from initialization, meaning $k=0$. 
Networks with weights from iteration $k > 0$ are called \textit{matching} \autocite{LinearModeConnectivity}, hence, $\theta_{match}$.
The Instability Analysis at initialization shows that the subnetworks are only stable if they are matching.
Subsequently, the authors apply Instability Analysis to multiple subnetworks created from the same dense network over multiple values for the reset iteration $k$.
These experiments show that the subnetworks that are not stable when $k=0$ become stable when they are rewound to a later iteration $k$.
The iteration $k$ when they get stable closely coincides with the iteration where the subnetworks become matching, which suggests a tight link between the two concepts.
The experiments were only done with dense networks and extremely sparse networks, namely between 1.5\% and 16.8\% for the smaller networks, 30\% for ResNet-50 and InceptionNet \autocite{LinearModeConnectivity}.

These results suggest that, at least with the current tools available, no actual winning tickets could be found for these deeper networks thus far.
However, progress was made from a different angle.

\subsection{Pruning Strategies for Lottery Tickets}
The original Lottery Ticket experiments use a simple pruning strategy of masking a certrain percentage of weights with the lowest magnitudes \autocite{DBLP:conf/iclr/FrankleC19}, as in \autocite{HanEtAl15}. This simple heuristic has yielded impressive results, yet there are many other possible heuristics one could image for such a task.

\textcite{DBLP:conf/nips/ZhouLLY19} perform an ablation study with regards to pruning strategies. Mask criteria are defined as functions that determine a score for each parameter. This score is used to rank the parameters and the bottom $p\%$ are masked.
The criteria use 


A variety of criteria is considered, including: 
\begin{itemize}
  \item magnitude at initialization \\
  mask the smallest (or largest) weights by their magnitude at initialization
  \item magnitude after training \\
  mask the smallest (or largest) weights by their magnitude after training
  \item magnitude at initialization and after training: mask the weights that had the smallest (or largest) magnitudes at initialization and after training
  \item magnitude increase \\
  mask the weights that have the highest decrease or smallest increase in magnitude after training
  \item movement \\
  mask the weights that, after training, are closest to value at initialization
  \item random \\
  mask randomly as a baseline
\end{itemize}

The authors followed the experimental setup of \textcite{DBLP:conf/iclr/FrankleC19} and evaluate with fully connected networks as well as convolutional neural networks. The simple magnitude pruning approach is among the best performing in the experiments. However, also pruning by movement produced well performing lottery tickets.

The results suggest that pruning the smallest weights of a network is a competetive strategy. Although it may not be the best strategy, as other alternatives produce competitive results, it definetly is relevant algorithm to use and study. 

\subsection{Theoretical Understanding of Iterative Magnitude Pruning}
\textcite{WhyLotteryTicketsWin} state a hypothesis as to why lottery tickets work. They hypothesize that lottery tickets effectively relearn the same solution as the one achieved by pruning alone, which they term 'pruned solution'.
The authors show that the lottery tickets are significantly closer to the subnetwork that was pruned solution than randomly reinitialized sparse models were after training. 
The experiments are conducted on the original LeNet5 architecture trained on MNIST, as well as ResNet-50 Architecture trained on ImageNet. Further they show that the trained lottery tickets reside in the same basin of the loss landscape as the pruned solution, by linearly interpolating between the two networks, in the same way as in \autocite{LinearModeConnectivity}.

\textcite{maene_towards_2021} name the hypothesis of \autocite{WhyLotteryTicketsWin} the regurgitating tickets interpretation.
They designed an experiment in which they force the network to learn a different solution by creating a repellent loss function.
This loss increases when the model is close to the same solution as in the previous iteration of IMP.
This forces the network to find a new solution every iteration.
With the repellent loss, no lottery tickets could be found, which further indicates that the solution found with IMP indeed relearns the pruned solution.

TODO what are the theories for why it works? learn to the same basin as sgd.

\section{Understanding Lottery Tickets}
\subsection{Numerical Characteristics}
\autocite{maene_towards_2021} note that the average magnitude of the unpruned weights increases with each pruning round. The smallest magnitude weights in the network are pruned, and the remaining weights train to similar values, therby increasing the average magnitude. The authors note, that this follows from the regurgitating tickets interpretation, described in chapter (?).
In the original lottery tickets paper by \textcite{DBLP:conf/iclr/FrankleC19}, the authors analyse the distribution of weight magnitudes after training. Before the first pruning iteration, the distribution of weight magnitudes follows a gaussian distribution. After several pruning iterations, the distributions tends towards a bimodal distribution, where the values close to zero are carved out.
The authors attempted to reinitialize the sparse subnetworks randomly according to the distribution of the the lottery ticket weight magnitudes, however the performance was only slightly better that reinitialization with the original weight distribution.

\subsection{Lottery Ticket as a Directed Acyclic Graph}
It can be interesting to view the lottery ticket as a directed Acyclic Graph (DAG).
Since it is sparsely connected, the remaining connections could provide insight into the funcionality of the network.
\textcite{DBLP:conf/iclr/FrankleC19} analyse the connectivity of each node in the Lottery Ticket DAG.
They find that the input connectivity, namely the number of incomming edges to a node (neuron) is relatively even among nodes of the same layer.
The connectivity examined in LeNet trained on MNIST is approximately proportional to the sparsity of the layer.
Regarding the output connections, there are bigger differences in connectivity amongst nodes.
Especially in the input layer, a significant proportion of nodes has no remaining outgoing connections at high sparsity. 
The authors hypothesize that the disconnected input nodes are not informative and likely correspond to the outer frame of the MNIST Image, which does not contain information about the desired prediction task.

When applying unstructured pruning (prune individual weights instead of neurons), some neurons may end up in a state where they do not have any incoming or outgoing connections. 

\textcite{HanEtAl15} acknoledge the possibility of 'dead neurons'. 
The authors state that dead neurons and all connections to or from a dead neuron can be safely pruned.
According to \autocite{HanEtAl15}, the dead neurons do not contribute to the final loss, and therefore do not receive any gradients. 
During retraining, regularization will remove the dead neurons automatically. 
A missing scenario is the existence of a neuron with outgoing connections, a bias and no incoming connections. This neuron might still contribute to the output and receive gradients. 
In this case, the neuron is considered dead, but removing it changes the output of the network.
\textcite{AllAlivePruning} include this scenario and describe that removing such a neuron maintains the networks function by simply transfering the bias to the next layer. 
Therefore, all neurons with no incoming or outgoing connections can be assumed to be 'dead', thus removable. Further, all weights connection to a dead neuron can be removed as well.
\textcite{AllAlivePruning} propose a novel pruning strategy employing this knowledge, named 'All-Alive-Pruning' . 
The authors use Iterative Magnitude Pruning. Dead neurons and the weights connected to them are removed after pruning, resulting in an 'all-alive' network.
Only at very high sparsities, the networks pruned with All-Alive-Pruning demonstrate better performance. 
This improvement is attributed to the increased capacity of the subnetwork, since more of its parameters are 'alive' and can contribute to the network output.

Only a few researchers looked at lottery tickets derived with Iterative Magnitude Pruning through the lens of Directed Acyclic Graphs. 
The insights relate the function of the network to its structure.
`Dead neurons' \autocite{HanEtAl15, AllAlivePruning} are neurons are part of the graph structure that can be removed without harming the function. 
\textcite{DBLP:conf/iclr/FrankleC19} discover that inputs that do not transmit information are pruned at higher rates, leaving them with less outgoing connections.
Overall the structure of Lottery Tickets and how it relates to its function remains elusive.

\subsection{BIMT - Brain inspired modular training}
A sparse network where the structure of the network relates to the underlying algorithm or problem structure can be beneficial for interpretability.
\textcite{BIMT} propose a training method called Brain-Inspired Modular Training (BIMT) that aims to produce sparse networks that can more easily be visually interpreted.
The authors aim to encourage modularity in the network through embedding the neurons in a euclidean space.
By punishing connections that are long in euclidean space, visible clusters of neurons form. 
Since neurons that are next to each other in the euclidean space do not necessarily belong into logical group, the authors introduce an additional algorithm that swaps neurons in the network to increase the total weighted length of connections.
On simple symbolic tasks, \autocite{BIMT} uncover structures that represent the structure of the underlying tasks.
On symbolic datasets, BIMT succeeds in discovering independence, feature sharing and compositionality of the problem, which is visible in the resulting structure of the sparse network.
The authors also use BIMT to train a feed forward neural network on simple classification tasks as well as the MNIST dataset, where they used three dimensional euclidean space to embed the network.
With larger models and especially in three dimensions, the shortcomings of the method become apparent as visual inspection does not help for interpretation any longer.

\section{Alternative Lottery Strategies}
several algorithms have been proposed to find lottery tickets. here is an inexaustive collection of some of them 

\subsection{Supermasks}
In follow-up work by \textcite{DBLP:conf/nips/ZhouLLY19}, the authors demonstrate that it is possible to find well-performing subnetworks at initialization by only pruning.
First, each weight in the network is assigned a score, which is 1 for every weight at the beginning. 
The scores are denoted $s$, where $|s| = |\theta|$.
In each forward pass of the network, the so-called \textit{effective weights} are used.
The effective weights $\theta_{eff}$ are the weights at initialization $\theta$ multiplied element-wise with a stochastically sampled mask.
This mask is sampled according to $m = \textit{Bern}(\sigma(s))$, where $\textit{Bern}(p)$ is a Bernoulli sampler producing a $1$ with probability $p$ and $\sigma(\cdot)$ is the sigmoid function. 
Thus, the effective weights are $\theta_{eff} = \textit{Bern}(\sigma(s)) \odot \theta$.
The scores are the learned parameters and are updated via backpropagation.
Experiments were conducted with a FCNN on MNIST and a CNN on CIFAR-10, reaching 95.3\% and 65.4\% test accuracy, respectively.
The authors call these masks \textit{supermasks}.

\subsection{Edge Popup}
Based on those results, \textcite{DBLP:conf/cvpr/RamanujanWKFR20} scale up the experiments and modify the algorithm for producing the mask slightly.
They remove the stochastic sampling of the Bernoulli sampler, as they claim that the stochasticity limits the performance. 
Instead of sampling the mask values, a real-valued score is learned for each weight. 
The subnetwork is chosen by selecting the top-$k$\% highest scores in each layer.
The algorithm for finding this mask is named \textit{Edge-Popup}(EP).
Experiments are conducted on small FCNNs and CNNs with CIFAR-10 and on several ResNet architectures with ImageNet.
Results show that even for the much harder ImageNet dataset, there exist randomly initialized subnetworks with non-trivial performance. 
An expressive finding of the experiments shows that a larger architecture (Wide-ResNet-50, 69M parameters) can reach 73.3\% accuracy on ImageNet by only pruning the weights with EP.
The resulting subnetwork has 20.6M parameters. In comparison, they show that this “pruning-only” approach can compete with a fully trained ResNet-34 (21.8M parameters) which also reaches 73.3\% accuracy. 
Surprisingly, the authors note that the subnetworks obtained by EP do not respond to any further training.
Furthermore, they note that the subnetwork does not train to the same accuracy as the dense network.
Therefore, the subnetworks are not considered winning tickets.
The results suggest that there is another phenomenon at play that makes these networks perform well \autocite{DBLP:conf/cvpr/RamanujanWKFR20}.
Despite that, the experiments show that there exist well-performing subnetworks at initialization, without any weight updates.

In addition, a variety of other methods \autocite{GraSP,SNIP,SynFlow} were introduced to prune networks at initialization. However, empirical investigations by \textcite{PruningAtInitMissingTheMark, SanityCheckingPruningMethods} show that none of these approaches perform better than carefully selecting layer-wise sparsities.
Based on this finding, \autocite{SanityCheckingPruningMethods} propose a method that select layer-wise sparsities called Smart-Ratio (SR).

\subsection{Rare Gems}
Drawing Inspiration from the EP-algorithm, \textcite{RareGems} propose an algorithm called Gem-Miner (GM) that overcomes the issues of the apparent lack of trainability of the obtained subnetworks.
The GM-algorithm assigns a normalized score to each weight. 
To obtain the mask $m$, a simple rounding function is applied to each score. The authors used a simple deterministic rounding function, where values below $0.5$ are rounded down to $0$ and all other values to $1$.
In the forward pass, the effective weights $\theta_{eff} = \theta \odot m$ are used, where $\theta$ are the untrained weights at initialization. 
The scores are the learned parameters and are updated via backpropagation.
Furthermore, the scores are renormalized to the range $[0,1]$ after they are updated, if necessary.
Additionally, a regularization term is added to the loss, which enforces sparsity on the scores. 
The authors use the $L_2$ norm of the scores and note that $L_1$ norm performs almost identically.
The final sparsity of the obtained subnetwork can be controlled by the regularization hyperparameter $\lambda$, which scales the regularization term.
 If $\lambda = 0$, there is no control over the final sparsity. 
 The authors have discovered, in line with \cite{DBLP:conf/cvpr/RamanujanWKFR20}, that the sparsity level remains close to 50\%, as apparently this is where the accuracy is maximized.

Experiments with different ResNet architectures on CIFAR-10, TinyImageNet \autocite{Tinyimagenet} and Caltech-101 \autocite{Caltech101} were conducted. 
The results show that the subnetworks obtained via the GM-algorithm, are indeed winning tickets, since they can then be trained and reach comparable accuracy to its dense counterpart.
Furthermore, the subnetworks have reasonably high accuracy \textit{before} any weight training, which earns them the name \textit{Rare Gems}. 
This makes the set of rare gems a subset of winning tickets. 
They are winning tickets, that already perform well before weight training.
The obtained subnetworks do not always outperform winning tickets obtained with IMP. 
However, there is a considerable improvement over IMP in terms of resource intensity.
The authors claim, that their algorithm is up to \textbf{19x} faster than the iterative approach with IMP. 
This increase in efficiency is compelling for repeated experiments.

\section{Conclusion}
The study of lottery tickets is a broad and diverse field. Even though theoretical studies were mentioned in the literature review, the majority of research conducted in this space is of empirical nature.
TODO review all the papers and cite all of them again.
A blind spot in the literature is the connection between the structure of a lottery ticket and its function. 

\section{Other possibly related stuff}
\begin{itemize}
    \item dynamic sparse training
    \item network distillation
    \item tinyML
\end{itemize}




