{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038708cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "n_sample = 500\n",
    "X, y = make_moons(n_samples=n_sample, noise=0.1)\n",
    "color = ['red', 'blue']\n",
    "\n",
    "for i in range(n_sample):\n",
    "    plt.scatter(X[i,0],X[i,1],color=color[y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc524005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "\n",
    "phase = \"toy_swap\"\n",
    "\n",
    "plot_log = 5000#50\n",
    "\n",
    "steps = 10001\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# create dataset\n",
    "\n",
    "d_in = 2\n",
    "d_out = 2\n",
    "\n",
    "n_sample = 200\n",
    "X, y = make_moons(n_samples=n_sample, noise=0.1)\n",
    "X = torch.tensor(X, dtype=torch.float, requires_grad=True)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "n_sample = 200\n",
    "X_test, y_test = make_moons(n_samples=n_sample, noise=0.1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float, requires_grad=True)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "width = 20\n",
    "depth = 3\n",
    "shp = [d_in, 20, 20, d_out]\n",
    "\n",
    "\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "class BioLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, in_fold=1, out_fold=1):\n",
    "        super(BioLinear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_fold = in_fold\n",
    "        self.out_fold = out_fold\n",
    "        assert in_dim % in_fold == 0\n",
    "        assert out_dim % out_fold == 0\n",
    "        #compute in_cor, shape: (in_dim)\n",
    "        in_dim_fold = int(in_dim/in_fold)\n",
    "        out_dim_fold = int(out_dim/out_fold)\n",
    "        self.in_coordinates = torch.tensor(list(np.linspace(1/(2*in_dim_fold), 1-1/(2*in_dim_fold), num=in_dim_fold))*in_fold, dtype=torch.float)\n",
    "        self.out_coordinates = torch.tensor(list(np.linspace(1/(2*out_dim_fold), 1-1/(2*out_dim_fold), num=out_dim_fold))*out_fold, dtype=torch.float)\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x.clone()\n",
    "        self.output = self.linear(x).clone()\n",
    "        return self.output\n",
    "\n",
    "\n",
    "class BioMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None, token_embedding=False, embedding_size=None):\n",
    "        super(BioMLP, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            if i == 0:\n",
    "                # for modular addition\n",
    "                #linear_list.append(BioLinear(shp[i], shp[i+1], in_fold=2))\n",
    "                # for regression\n",
    "                linear_list.append(BioLinear(shp[i], shp[i+1], in_fold=1))\n",
    "                \n",
    "            else:\n",
    "                linear_list.append(BioLinear(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        \n",
    "        \n",
    "        if token_embedding == True:\n",
    "            # embedding size: number of tokens * embedding dimension\n",
    "            self.embedding = torch.nn.Parameter(torch.normal(0,1,size=embedding_size))\n",
    "        \n",
    "        self.shp = shp\n",
    "        # parameters for the bio-inspired trick\n",
    "        self.l0 = 0.2 # distance between two nearby layers\n",
    "        self.in_perm = torch.nn.Parameter(torch.tensor(np.arange(int(self.in_dim/self.linears[0].in_fold)), dtype=torch.float))\n",
    "        #self.register_parameter(name='in_perm', param=torch.nn.Parameter(torch.tensor(np.arange(int(self.in_dim/self.linears[0].in_fold)), dtype=torch.float)))\n",
    "        self.out_perm = torch.nn.Parameter(torch.tensor(np.arange(int(self.out_dim/self.linears[-1].out_fold)), dtype=torch.float))\n",
    "        #self.register_parameter(name='out_perm', param=torch.nn.Parameter(torch.tensor(np.arange(int(self.out_dim/self.linears[-1].out_fold)), dtype=torch.float)))\n",
    "        self.top_k = 10\n",
    "        self.token_embedding = token_embedding\n",
    "        self.n_parameters = sum(p.numel() for p in self.parameters())\n",
    "        self.original_params = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shp = x.shape\n",
    "        in_fold = self.linears[0].in_fold\n",
    "        x = x.reshape(shp[0], in_fold, int(shp[1]/in_fold))\n",
    "        x = x[:,:,self.in_perm.long()]\n",
    "        x = x.reshape(shp[0], shp[1])\n",
    "        f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        out_perm_inv = torch.zeros(self.out_dim, dtype=torch.long)\n",
    "        out_perm_inv[self.out_perm.long()] = torch.arange(self.out_dim)\n",
    "        x = x[:,out_perm_inv]\n",
    "        #x = x[:,self.out_perm]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_linear_layers(self):\n",
    "        return self.linears\n",
    "    \n",
    "    def get_cc(self, weight_factor=1.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # compute connection cost\n",
    "        cc = 0\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0))\n",
    "            cc += torch.sum(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.sum(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.sum(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        return cc\n",
    "    \n",
    "    def swap_weight(self, weights, j, k, swap_type=\"out\"):\n",
    "        with torch.no_grad():  \n",
    "            if swap_type == \"in\":\n",
    "                temp = weights[:,j].clone()\n",
    "                weights[:,j] = weights[:,k].clone()\n",
    "                weights[:,k] = temp\n",
    "            elif swap_type == \"out\":\n",
    "                temp = weights[j].clone()\n",
    "                weights[j] = weights[k].clone()\n",
    "                weights[k] = temp\n",
    "            else:\n",
    "                raise Exception(\"Swap type {} is not recognized!\".format(swap_type))\n",
    "            \n",
    "    def swap_bias(self, biases, j, k):\n",
    "        with torch.no_grad():  \n",
    "            temp = biases[j].clone()\n",
    "            biases[j] = biases[k].clone()\n",
    "            biases[k] = temp\n",
    "    \n",
    "    def swap(self, i, j, k):\n",
    "        # in the ith layer (of neurons), swap the jth and the kth neuron. \n",
    "        # Note: n layers of weights means n+1 layers of neurons.\n",
    "        # (incoming, outgoing) * weights + biases are swapped. \n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer, only has outgoing weights; update in_perm\n",
    "            weights = linears[i].linear.weight\n",
    "            infold = linears[i].in_fold\n",
    "            fold_dim = int(weights.shape[1]/infold)\n",
    "            for l in range(infold):\n",
    "                self.swap_weight(weights, j+fold_dim*l, k+fold_dim*l, swap_type=\"in\")\n",
    "            # change input_perm\n",
    "            self.swap_bias(self.in_perm, j, k)\n",
    "        elif i == num_linear:\n",
    "            # output layer, only has incoming weights and biases; update out_perm\n",
    "            weights = linears[i-1].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights, j, k, swap_type=\"out\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "            # change output_perm\n",
    "            self.swap_bias(self.out_perm, j, k)\n",
    "        else:\n",
    "            # middle layer : (incoming, outgoing) * weights, and biases\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights_in, j, k, swap_type=\"out\")\n",
    "            self.swap_weight(weights_out, j, k, swap_type=\"in\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "\n",
    "    def get_top_id(self, i, top_k=20):\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer\n",
    "            weights = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=0)\n",
    "            in_fold = linears[0].in_fold\n",
    "            #print(score.shape)\n",
    "            score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "        elif i == num_linear:\n",
    "            # output layer\n",
    "            weights = linears[i-1].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=1)\n",
    "        else:\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "        #print(score.shape)\n",
    "        top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "        return top_index\n",
    "    \n",
    "    def relocate_ij(self, i, j):\n",
    "        # In the ith layer (of neurons), relocate the jth neuron\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i < num_linear:\n",
    "            num_neuron = int(linears[i].linear.weight.shape[1]/linears[i].in_fold)\n",
    "        else:\n",
    "            num_neuron = linears[i-1].linear.weight.shape[0]\n",
    "        ccs = []\n",
    "        for k in range(num_neuron):\n",
    "            self.swap(i,j,k)\n",
    "            ccs.append(self.get_cc())\n",
    "            self.swap(i,j,k)\n",
    "        k = torch.argmin(torch.stack(ccs))\n",
    "        self.swap(i,j,k)\n",
    "            \n",
    "    def relocate_i(self, i):\n",
    "        # Relocate neurons in the ith layer\n",
    "        top_id = self.get_top_id(i, top_k=self.top_k)\n",
    "        for j in top_id:\n",
    "            self.relocate_ij(i,j)\n",
    "            \n",
    "    def relocate(self):\n",
    "        # Relocate neurons in the whole model\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        for i in range(num_linear+1):\n",
    "            self.relocate_i(i)\n",
    "            \n",
    "    def plot(self):\n",
    "        #fig, ax = plt.subplots(figsize=(6,6))\n",
    "        ax = plt.gca()\n",
    "        shp = self.shp\n",
    "        s = 1/(2*max(shp))\n",
    "        for j in range(len(shp)):\n",
    "            N = shp[j]\n",
    "            if j == 0:\n",
    "                in_fold = self.linears[j].in_fold\n",
    "                N = int(N/in_fold)\n",
    "            for i in range(N):\n",
    "                if j == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j+0.02*fold-0.01), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "                else:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "\n",
    "\n",
    "        plt.ylim(-0.02,0.1*(len(shp)-1)+0.02)\n",
    "        plt.xlim(-0.02,1.02)\n",
    "\n",
    "        linears = self.linears\n",
    "        for ii in range(len(linears)):\n",
    "            biolinear = linears[ii]\n",
    "            p = biolinear.linear.weight\n",
    "            p_shp = p.shape\n",
    "            p = p/torch.abs(p).max()\n",
    "            in_fold = biolinear.in_fold\n",
    "            fold_num = int(p_shp[1]/in_fold)\n",
    "            for i in range(p_shp[0]):\n",
    "                if ii == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        for j in range(fold_num):\n",
    "                            plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii+0.02*fold-0.01], lw=1*np.abs(p[i,j].detach().numpy()), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                else:\n",
    "                    for j in range(fold_num):\n",
    "                        plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii], lw=0.5*np.abs(p[i,j].detach().numpy()), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                    \n",
    "        ax.axis('off')\n",
    "        \n",
    "    def thresholding(self, threshold, checkpoint = True):\n",
    "        num = 0\n",
    "        if checkpoint:\n",
    "            self.original_params = [param.clone() for param in self.parameters()]\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                num += torch.sum(torch.abs(param)>threshold)\n",
    "                param.data = param*(torch.abs(param)>threshold)\n",
    "        return num\n",
    "                \n",
    "    def intervening(self, i, pos, value, ptype=\"weight\", checkpoint = True):\n",
    "        if checkpoint:\n",
    "            self.original_params = [param.clone() for param in self.parameters()]\n",
    "        with torch.no_grad():\n",
    "            if ptype == \"weight\":\n",
    "                self.linears[i].linear.weight[pos] = value\n",
    "            elif ptype == \"bias\":\n",
    "                self.linears[i].linear.bias[pos] = value\n",
    "                \n",
    "    def revert(self):\n",
    "        with torch.no_grad():\n",
    "            for param, original_param in zip(self.parameters(), self.original_params):\n",
    "                param.data.copy_(original_param.data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92992b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BioMLP(shp=[2,20,20,2])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "#steps = 10000\n",
    "log = 200\n",
    "lamb = 0.001\n",
    "swap_log = 200\n",
    "\n",
    "\n",
    "# START TRAINING\n",
    "for step in range(steps):\n",
    "    \n",
    "    # small lambda first, then large lambda, then small lambda\n",
    "    if step == 5000:\n",
    "        lamb = 0.01\n",
    "        \n",
    "    if step == 15000:\n",
    "        lamb = 0.001\n",
    "    \n",
    "    CEL = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred  = model(X)\n",
    "    loss = CEL(pred, y)\n",
    "    #print((torch.argmax(pred, dim=1) == y).float())\n",
    "    acc = torch.mean((torch.argmax(pred, dim=1) == y).float())\n",
    "    \n",
    "    \n",
    "    pred_test  = model(X_test)\n",
    "    loss_test = CEL(pred_test, y_test)\n",
    "    acc_test = torch.mean((torch.argmax(pred_test, dim=1) == y_test).float())\n",
    "    \n",
    "    reg = model.get_cc(weight_factor=1.)\n",
    "    total_loss = loss + lamb*reg\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | total loss: %.2e | train loss: %.2e | test loss %.2e | train acc : %.2f | test acc : %.2f | reg: %.2e \"%(step, total_loss.detach().numpy(), loss.detach().numpy(), loss_test.detach().numpy(), acc.detach().numpy(), acc_test.detach().numpy(), reg.detach().numpy()))\n",
    "    \n",
    "    if (step+1) % swap_log == 0:\n",
    "        #pass\n",
    "        model.relocate()\n",
    "\n",
    "\n",
    "    if step % plot_log == 0:\n",
    "        plt.figure(figsize=(3, 7)) \n",
    "\n",
    "        plt.subplot(2,1,1)\n",
    "\n",
    "        N = 2\n",
    "        s = 1/(2*max(shp))\n",
    "        for j in range(len(shp)):\n",
    "            N = shp[j]\n",
    "            for i in range(N):\n",
    "                circle = Ellipse((1/(2*N)+i/N, 0.1*j), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                plt.gca().add_patch(circle)\n",
    "\n",
    "\n",
    "        plt.ylim(-0.02,0.1*(len(shp)-1)+0.02)\n",
    "        plt.xlim(-0.02,1.02)\n",
    "\n",
    "        ii = 0\n",
    "        for p in model.parameters():\n",
    "\n",
    "\n",
    "            if len(p.shape) == 2:\n",
    "                p_shp = p.shape\n",
    "                p = p/torch.abs(p).max()\n",
    "                for i in range(p_shp[0]):\n",
    "                    for j in range(p_shp[1]):\n",
    "                        if p[i,j] > 0:\n",
    "                            plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*p_shp[1])+j/p_shp[1]], [0.1*(ii+1),0.1*ii], lw=1*np.abs(p[i,j].detach().numpy()), color=\"blue\")\n",
    "                        else:\n",
    "                            plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*p_shp[1])+j/p_shp[1]], [0.1*(ii+1),0.1*ii], lw=1*np.abs(p[i,j].detach().numpy()), color=\"red\")\n",
    "\n",
    "                formulas = [\"Class 1\", \"Class 2\"]\n",
    "                if ii == 0:\n",
    "                    for j in range(p_shp[1]):\n",
    "                        plt.text(1/(2*p_shp[1])+j/p_shp[1]-0.05, 0.1*ii-0.04, \"$x_{}$\".format(model.in_perm[j].long()+1), fontsize=15)\n",
    "                ii += 1\n",
    "\n",
    "\n",
    "        for j in range(p_shp[0]):\n",
    "            plt.text(1/(2*p_shp[0])+j/p_shp[0]-0.15, 0.1*ii+0.02, formulas[model.out_perm[j].long()], fontsize=15)\n",
    "\n",
    "        plt.gca().axis('off')\n",
    "        plt.title(\"step={}\".format(step), fontsize=15, y=1.1)\n",
    "\n",
    "\n",
    "        plt.subplot(2,1,2)\n",
    "\n",
    "\n",
    "        start_x = X[:,0].min()-0.1\n",
    "        end_x = X[:,0].max()+0.1\n",
    "        start_y = X[:,1].min()-0.1\n",
    "        end_y = X[:,1].max()+0.1\n",
    "        n_values = 30\n",
    "\n",
    "        x_vals = np.linspace(start_x.detach().numpy(), end_x.detach().numpy(), n_values)\n",
    "        y_vals = np.linspace(start_y.detach().numpy(), end_y.detach().numpy(), n_values)\n",
    "        XX, YY = np.meshgrid(x_vals, y_vals)\n",
    "        pred = model(torch.tensor([XX.reshape(-1,), YY.reshape(-1,)], dtype=torch.float).permute(1,0))\n",
    "        pred = pred[:,1] - pred[:,0]\n",
    "\n",
    "        #ZZ = np.sqrt(XX**2 + YY**2)\n",
    "\n",
    "        cp = plt.contourf(XX, YY, pred.reshape(n_values,n_values).detach().numpy(), [-100,0.,100.], colors=[\"green\",\"orange\"], alpha=0.2)\n",
    "        #plt.colorbar(cp)\n",
    "        color = ['green', 'orange']\n",
    "\n",
    "        for i in range(n_sample):\n",
    "            plt.scatter(X[i,0].detach().numpy(),X[i,1].detach().numpy(),color=color[y[i]])\n",
    "\n",
    "\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=15)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=15)\n",
    "        #plt.show()\n",
    "    \n",
    "        #plt.savefig(\"./results/two_moon/steps/{0:05d}.png\".format(step))\n",
    "        \n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ca492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
