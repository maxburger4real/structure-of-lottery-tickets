{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "\n",
    "from datasets.symbolic_1 import get_dataloaders, INPUT_DIM, OUTPUT_DIM\n",
    "from architectures import BioMLP\n",
    "from nx_utils import STATE_DICT\n",
    "import pathlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL BIMT HYPERPARAMETERS\n",
    "shp = [INPUT_DIM, 20, 20, OUTPUT_DIM]\n",
    "model = BioMLP(shp=shp)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "log = 200\n",
    "lamb = 0.001\n",
    "dump_every = 200\n",
    "swap_log = 200\n",
    "weight_factor = 1.\n",
    "plot_log = 50\n",
    "epochs = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL BIMT HYPERPARAMETERS\n",
    "shp = [INPUT_DIM, 20, 20, OUTPUT_DIM]\n",
    "model = BioMLP(shp=shp)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "log = 200\n",
    "lamb = 0.001\n",
    "dump_every = 50\n",
    "swap_log = 200\n",
    "weight_factor = 1.\n",
    "plot_log = 50\n",
    "epochs = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('models/BioMLP_4_20_20_2/2023-09-17_164345')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = BioMLP.__name__ + str(shp).replace(', ','_').replace('[','_').replace(']','')\n",
    "base = pathlib.Path(\"models\") / name / datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "base.mkdir(parents=True, exist_ok=True)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | total loss: 1.22e+00 | train loss: 1.19e+00 | test loss 1.75e+00 | reg: 2.92e+01 \n",
      "step = 200 | total loss: 2.85e-01 | train loss: 2.50e-01 | test loss 6.43e-01 | reg: 3.50e+01 \n",
      "step = 400 | total loss: 2.18e-01 | train loss: 1.85e-01 | test loss 5.32e-01 | reg: 3.28e+01 \n",
      "step = 600 | total loss: 8.72e-02 | train loss: 5.01e-02 | test loss 1.50e-01 | reg: 3.72e+01 \n",
      "step = 800 | total loss: 4.09e-02 | train loss: 6.10e-03 | test loss 3.76e-02 | reg: 3.48e+01 \n",
      "step = 1000 | total loss: 3.10e-02 | train loss: 2.64e-03 | test loss 2.29e-02 | reg: 2.84e+01 \n",
      "step = 1200 | total loss: 2.66e-02 | train loss: 1.91e-03 | test loss 1.81e-02 | reg: 2.47e+01 \n",
      "step = 1400 | total loss: 2.31e-02 | train loss: 1.54e-03 | test loss 1.56e-02 | reg: 2.15e+01 \n",
      "step = 1600 | total loss: 1.99e-02 | train loss: 1.21e-03 | test loss 1.47e-02 | reg: 1.87e+01 \n",
      "step = 1800 | total loss: 1.75e-02 | train loss: 8.88e-04 | test loss 1.39e-02 | reg: 1.66e+01 \n",
      "step = 2000 | total loss: 1.58e-02 | train loss: 6.89e-04 | test loss 1.37e-02 | reg: 1.51e+01 \n",
      "step = 2200 | total loss: 1.48e-02 | train loss: 6.03e-04 | test loss 1.44e-02 | reg: 1.42e+01 \n",
      "step = 2400 | total loss: 1.38e-02 | train loss: 5.46e-04 | test loss 1.47e-02 | reg: 1.33e+01 \n",
      "step = 2600 | total loss: 1.31e-02 | train loss: 4.81e-04 | test loss 1.51e-02 | reg: 1.26e+01 \n",
      "step = 2800 | total loss: 1.22e-02 | train loss: 4.46e-04 | test loss 1.58e-02 | reg: 1.17e+01 \n",
      "step = 3000 | total loss: 1.16e-02 | train loss: 4.27e-04 | test loss 1.63e-02 | reg: 1.12e+01 \n",
      "step = 3200 | total loss: 1.11e-02 | train loss: 4.06e-04 | test loss 1.67e-02 | reg: 1.06e+01 \n",
      "step = 3400 | total loss: 1.06e-02 | train loss: 3.86e-04 | test loss 1.70e-02 | reg: 1.02e+01 \n",
      "step = 3600 | total loss: 1.02e-02 | train loss: 3.87e-04 | test loss 1.73e-02 | reg: 9.79e+00 \n",
      "step = 3800 | total loss: 9.88e-03 | train loss: 3.78e-04 | test loss 1.71e-02 | reg: 9.51e+00 \n",
      "step = 4000 | total loss: 9.50e-03 | train loss: 3.76e-04 | test loss 1.70e-02 | reg: 9.12e+00 \n",
      "step = 4200 | total loss: 9.03e-03 | train loss: 3.78e-04 | test loss 1.68e-02 | reg: 8.65e+00 \n",
      "step = 4400 | total loss: 8.68e-03 | train loss: 3.77e-04 | test loss 1.67e-02 | reg: 8.30e+00 \n",
      "step = 4600 | total loss: 8.41e-03 | train loss: 3.72e-04 | test loss 1.67e-02 | reg: 8.04e+00 \n",
      "step = 4800 | total loss: 8.10e-03 | train loss: 3.77e-04 | test loss 1.67e-02 | reg: 7.72e+00 \n",
      "step = 5000 | total loss: 7.31e-02 | train loss: 3.79e-04 | test loss 1.63e-02 | reg: 7.27e+00 \n",
      "step = 5200 | total loss: 6.04e-02 | train loss: 3.37e-03 | test loss 3.41e-02 | reg: 5.70e+00 \n",
      "step = 5400 | total loss: 5.92e-02 | train loss: 3.17e-03 | test loss 3.36e-02 | reg: 5.60e+00 \n",
      "step = 5600 | total loss: 5.86e-02 | train loss: 3.03e-03 | test loss 3.33e-02 | reg: 5.56e+00 \n",
      "step = 5800 | total loss: 5.83e-02 | train loss: 2.88e-03 | test loss 3.31e-02 | reg: 5.54e+00 \n",
      "step = 6000 | total loss: 5.80e-02 | train loss: 2.76e-03 | test loss 3.29e-02 | reg: 5.52e+00 \n",
      "step = 6200 | total loss: 5.78e-02 | train loss: 2.65e-03 | test loss 3.27e-02 | reg: 5.51e+00 \n",
      "step = 6400 | total loss: 5.76e-02 | train loss: 2.56e-03 | test loss 3.28e-02 | reg: 5.51e+00 \n",
      "step = 6600 | total loss: 5.75e-02 | train loss: 2.47e-03 | test loss 3.27e-02 | reg: 5.51e+00 \n",
      "step = 6800 | total loss: 5.74e-02 | train loss: 2.43e-03 | test loss 3.24e-02 | reg: 5.50e+00 \n",
      "step = 7000 | total loss: 5.73e-02 | train loss: 2.38e-03 | test loss 3.25e-02 | reg: 5.50e+00 \n",
      "step = 7200 | total loss: 5.73e-02 | train loss: 2.33e-03 | test loss 3.24e-02 | reg: 5.49e+00 \n",
      "step = 7400 | total loss: 5.72e-02 | train loss: 2.30e-03 | test loss 3.24e-02 | reg: 5.49e+00 \n",
      "step = 7600 | total loss: 5.72e-02 | train loss: 2.26e-03 | test loss 3.25e-02 | reg: 5.49e+00 \n",
      "step = 7800 | total loss: 5.71e-02 | train loss: 2.24e-03 | test loss 3.24e-02 | reg: 5.49e+00 \n",
      "step = 8000 | total loss: 5.71e-02 | train loss: 2.23e-03 | test loss 3.23e-02 | reg: 5.49e+00 \n",
      "step = 8200 | total loss: 5.71e-02 | train loss: 2.20e-03 | test loss 3.22e-02 | reg: 5.49e+00 \n",
      "step = 8400 | total loss: 5.71e-02 | train loss: 2.19e-03 | test loss 3.24e-02 | reg: 5.49e+00 \n",
      "step = 8600 | total loss: 5.70e-02 | train loss: 2.18e-03 | test loss 3.24e-02 | reg: 5.49e+00 \n",
      "step = 8800 | total loss: 5.70e-02 | train loss: 2.16e-03 | test loss 3.23e-02 | reg: 5.49e+00 \n",
      "step = 9000 | total loss: 5.70e-02 | train loss: 2.13e-03 | test loss 3.22e-02 | reg: 5.49e+00 \n",
      "step = 9200 | total loss: 5.70e-02 | train loss: 2.13e-03 | test loss 3.24e-02 | reg: 5.49e+00 \n",
      "step = 9400 | total loss: 5.70e-02 | train loss: 2.13e-03 | test loss 3.23e-02 | reg: 5.49e+00 \n",
      "step = 9600 | total loss: 5.70e-02 | train loss: 2.12e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 9800 | total loss: 5.70e-02 | train loss: 2.12e-03 | test loss 3.22e-02 | reg: 5.49e+00 \n",
      "step = 10000 | total loss: 5.70e-02 | train loss: 2.11e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 10200 | total loss: 5.70e-02 | train loss: 2.10e-03 | test loss 3.21e-02 | reg: 5.49e+00 \n",
      "step = 10400 | total loss: 5.70e-02 | train loss: 2.10e-03 | test loss 3.21e-02 | reg: 5.49e+00 \n",
      "step = 10600 | total loss: 5.70e-02 | train loss: 2.08e-03 | test loss 3.21e-02 | reg: 5.49e+00 \n",
      "step = 10800 | total loss: 5.70e-02 | train loss: 2.08e-03 | test loss 3.21e-02 | reg: 5.49e+00 \n",
      "step = 11000 | total loss: 5.70e-02 | train loss: 2.08e-03 | test loss 3.20e-02 | reg: 5.49e+00 \n",
      "step = 11200 | total loss: 5.69e-02 | train loss: 2.07e-03 | test loss 3.22e-02 | reg: 5.49e+00 \n",
      "step = 11400 | total loss: 5.70e-02 | train loss: 2.07e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 11600 | total loss: 5.69e-02 | train loss: 2.07e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 11800 | total loss: 5.69e-02 | train loss: 2.08e-03 | test loss 3.20e-02 | reg: 5.49e+00 \n",
      "step = 12000 | total loss: 5.69e-02 | train loss: 2.07e-03 | test loss 3.20e-02 | reg: 5.49e+00 \n",
      "step = 12200 | total loss: 5.69e-02 | train loss: 2.06e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 12400 | total loss: 5.69e-02 | train loss: 2.06e-03 | test loss 3.20e-02 | reg: 5.49e+00 \n",
      "step = 12600 | total loss: 5.69e-02 | train loss: 2.06e-03 | test loss 3.19e-02 | reg: 5.48e+00 \n",
      "step = 12800 | total loss: 5.69e-02 | train loss: 2.06e-03 | test loss 3.19e-02 | reg: 5.48e+00 \n",
      "step = 13000 | total loss: 5.69e-02 | train loss: 2.05e-03 | test loss 3.18e-02 | reg: 5.49e+00 \n",
      "step = 13200 | total loss: 5.69e-02 | train loss: 2.06e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 13400 | total loss: 5.69e-02 | train loss: 2.05e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 13600 | total loss: 5.69e-02 | train loss: 2.04e-03 | test loss 3.21e-02 | reg: 5.49e+00 \n",
      "step = 13800 | total loss: 5.69e-02 | train loss: 2.04e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 14000 | total loss: 5.69e-02 | train loss: 2.04e-03 | test loss 3.18e-02 | reg: 5.49e+00 \n",
      "step = 14200 | total loss: 5.69e-02 | train loss: 2.05e-03 | test loss 3.19e-02 | reg: 5.48e+00 \n",
      "step = 14400 | total loss: 5.69e-02 | train loss: 2.03e-03 | test loss 3.19e-02 | reg: 5.49e+00 \n",
      "step = 14600 | total loss: 5.69e-02 | train loss: 2.03e-03 | test loss 3.20e-02 | reg: 5.49e+00 \n",
      "step = 14800 | total loss: 5.69e-02 | train loss: 2.04e-03 | test loss 3.18e-02 | reg: 5.48e+00 \n",
      "step = 15000 | total loss: 7.52e-03 | train loss: 2.03e-03 | test loss 3.20e-02 | reg: 5.49e+00 \n",
      "step = 15200 | total loss: 7.61e-03 | train loss: 4.67e-04 | test loss 1.64e-02 | reg: 7.14e+00 \n",
      "step = 15400 | total loss: 7.55e-03 | train loss: 3.94e-04 | test loss 1.46e-02 | reg: 7.15e+00 \n",
      "step = 15600 | total loss: 7.52e-03 | train loss: 3.74e-04 | test loss 1.44e-02 | reg: 7.14e+00 \n",
      "step = 15800 | total loss: 7.50e-03 | train loss: 3.65e-04 | test loss 1.44e-02 | reg: 7.13e+00 \n",
      "step = 16000 | total loss: 7.48e-03 | train loss: 3.60e-04 | test loss 1.44e-02 | reg: 7.12e+00 \n",
      "step = 16200 | total loss: 7.46e-03 | train loss: 3.58e-04 | test loss 1.44e-02 | reg: 7.11e+00 \n",
      "step = 16400 | total loss: 7.45e-03 | train loss: 3.58e-04 | test loss 1.44e-02 | reg: 7.09e+00 \n",
      "step = 16600 | total loss: 7.44e-03 | train loss: 3.57e-04 | test loss 1.45e-02 | reg: 7.08e+00 \n",
      "step = 16800 | total loss: 7.43e-03 | train loss: 3.55e-04 | test loss 1.45e-02 | reg: 7.07e+00 \n",
      "step = 17000 | total loss: 7.42e-03 | train loss: 3.55e-04 | test loss 1.44e-02 | reg: 7.06e+00 \n",
      "step = 17200 | total loss: 7.41e-03 | train loss: 3.57e-04 | test loss 1.45e-02 | reg: 7.05e+00 \n",
      "step = 17400 | total loss: 7.40e-03 | train loss: 3.54e-04 | test loss 1.45e-02 | reg: 7.05e+00 \n",
      "step = 17600 | total loss: 7.39e-03 | train loss: 3.54e-04 | test loss 1.45e-02 | reg: 7.04e+00 \n",
      "step = 17800 | total loss: 7.39e-03 | train loss: 3.55e-04 | test loss 1.44e-02 | reg: 7.03e+00 \n",
      "step = 18000 | total loss: 7.38e-03 | train loss: 3.55e-04 | test loss 1.44e-02 | reg: 7.02e+00 \n",
      "step = 18200 | total loss: 7.37e-03 | train loss: 3.55e-04 | test loss 1.45e-02 | reg: 7.02e+00 \n",
      "step = 18400 | total loss: 7.37e-03 | train loss: 3.55e-04 | test loss 1.44e-02 | reg: 7.01e+00 \n",
      "step = 18600 | total loss: 7.36e-03 | train loss: 3.56e-04 | test loss 1.44e-02 | reg: 7.01e+00 \n",
      "step = 18800 | total loss: 7.36e-03 | train loss: 3.56e-04 | test loss 1.44e-02 | reg: 7.00e+00 \n",
      "step = 19000 | total loss: 7.36e-03 | train loss: 3.57e-04 | test loss 1.44e-02 | reg: 7.00e+00 \n",
      "step = 19200 | total loss: 7.35e-03 | train loss: 3.55e-04 | test loss 1.44e-02 | reg: 7.00e+00 \n",
      "step = 19400 | total loss: 7.35e-03 | train loss: 3.56e-04 | test loss 1.44e-02 | reg: 6.99e+00 \n",
      "step = 19600 | total loss: 7.35e-03 | train loss: 3.56e-04 | test loss 1.44e-02 | reg: 6.99e+00 \n",
      "step = 19800 | total loss: 7.34e-03 | train loss: 3.59e-04 | test loss 1.45e-02 | reg: 6.99e+00 \n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_dataloaders()\n",
    "for step in range(epochs):\n",
    "    \n",
    "    if step == int(epochs/4):\n",
    "        lamb *= 10\n",
    "    \n",
    "    if step == int(3*epochs/4):\n",
    "        lamb *= 0.1\n",
    "\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        pred_test  = model(x)\n",
    "        loss_test = torch.mean((pred_test-y)**2)\n",
    "        \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred  = model(x)\n",
    "        loss = torch.mean((pred-y)**2)\n",
    "\n",
    "        # do not penalize bias at first (this makes the weight graph look better)\n",
    "        training_in_last_quarter = step > int(3*epochs/4)\n",
    "        penalize = True if training_in_last_quarter else False\n",
    "        reg = model.get_cc(bias_penalize=penalize, weight_factor=weight_factor)\n",
    "\n",
    "        #reg = model.get_cc(bias_penalize=True)\n",
    "        total_loss = loss + lamb*reg\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | total loss: %.2e | train loss: %.2e | test loss %.2e | reg: %.2e \"%(step, total_loss.detach().numpy(), loss.detach().numpy(), loss_test.detach().numpy(), reg.detach().numpy()))\n",
    "    \n",
    "    if step % swap_log == 0:\n",
    "    #if (step+1) % swap_log == 0:\n",
    "        # TODO: this results in large weights for one epoch. WHY?\n",
    "        model.relocate()\n",
    "\n",
    "    if step % dump_every == 0:\n",
    "        torch.save({\n",
    "                'epoch': step,\n",
    "                STATE_DICT: model.state_dict(),\n",
    "                # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': loss.item(),\n",
    "                'test_loss': loss_test.item(),\n",
    "                }, base / f\"{step}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from nx_utils import( \n",
    "    get_weights_and_biases_from_state_dict, \n",
    "    load_state_dict_from_file, \n",
    "    get_shape_from_state_dict,\n",
    "    add_neuron_nodes,\n",
    "    black_and_white,\n",
    "    add_weight_edges_arrays,\n",
    "    get_layers_of_nodes,\n",
    "    layerwise_normalized_abs_value\n",
    ")\n",
    "from bokeh_utils import (\n",
    "    draw_interactive_mlp_graph,\n",
    "    LINE_COLOR_LIST,\n",
    "    LINE_WIDTH_LIST\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = sorted(base.glob(\"*.pt\"))\n",
    "state_dict = load_state_dict_from_file(list_of_files[0])\n",
    "weight_shapes = get_shape_from_state_dict(state_dict)\n",
    "\n",
    "# retrieve weights from state_dicts\n",
    "weights = [\n",
    "    get_weights_and_biases_from_state_dict(load_state_dict_from_file(f))[0]\n",
    "    for f in list_of_files\n",
    "]\n",
    "\n",
    "# reshape weights to include \"time\"-dimension as the first dim. \n",
    "layers_of_weights = [torch.stack(x) for x in zip(*weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add functions with names, which add attributes to the graph\n",
    "attribute_functions = {\n",
    "       LINE_COLOR_LIST : black_and_white(layers_of_weights),\n",
    "       LINE_WIDTH_LIST : layerwise_normalized_abs_value(layers_of_weights),\n",
    "    }\n",
    "\n",
    "# create a graph and populate with nodes\n",
    "G = nx.DiGraph()\n",
    "add_neuron_nodes(G, weight_shapes)\n",
    "add_weight_edges_arrays(G, get_layers_of_nodes(G), attribute_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# draw the network with bokeh\n",
    "draw_interactive_mlp_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
