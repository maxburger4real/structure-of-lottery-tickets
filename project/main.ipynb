{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from training_pipelines import imp\n",
    "from training_pipelines import regular\n",
    "\n",
    "from common.torch_utils import get_pytorch_device\n",
    "from common.architectures import SimpleMLP\n",
    "from common.datasets.independence import INPUT_DIM, OUTPUT_DIM, DATASET_NAME, build_loaders\n",
    "from common.tracking import Config, SGD, ADAM, PROJECT, save_hparams\n",
    "from common.training import build_optimizer, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/max/Documents/JKU/_Master_Thesis/project/wandb/run-20231006_131410-zq8ksc12</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mxmn/init-thesis/runs/zq8ksc12' target=\"_blank\">IMP-reinit-with nograd</a></strong> to <a href='https://wandb.ai/mxmn/init-thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mxmn/init-thesis' target=\"_blank\">https://wandb.ai/mxmn/init-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mxmn/init-thesis/runs/zq8ksc12' target=\"_blank\">https://wandb.ai/mxmn/init-thesis/runs/zq8ksc12</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▃▁▆▄▄▂▇▄▅▂▇▇▅▃██▆▃▃▁▆▄▄▂▇▇▅▂▇▇▅▃▃█▆▃▃▁▆▆</td></tr><tr><td>loss/eval/0.00</td><td>█▇▆▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.10</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.20</td><td>█▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.28</td><td>█▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.36</td><td>█▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.42</td><td>█▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.49</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.54</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.59</td><td>█▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.63</td><td>█▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.67</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.71</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.74</td><td>█▇▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.77</td><td>█▇▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.79</td><td>█▇▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.82</td><td>█▇▆▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.83</td><td>█▇▆▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.85</td><td>██▇▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.86</td><td>██▇▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.88</td><td>██▇▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.89</td><td>██▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.90</td><td>██▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.91</td><td>██▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.92</td><td>███▆▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.93</td><td>██▆▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁██▆▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.94</td><td>███▇▇▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval/0.95</td><td>██▅▅▂▂▂▁▁▁▁▁▁██▅▅▅▄▃▂▁▁▁▁▁██▅▅▅▅▅▄▃▃▂▂▂▂</td></tr><tr><td>loss/eval/0.96</td><td>███▇▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.00</td><td>█▇▆▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.10</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.20</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.28</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.36</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.42</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.49</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.54</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.59</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.63</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.67</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.71</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.74</td><td>█▆▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.77</td><td>█▇▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.79</td><td>█▇▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.82</td><td>█▇▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.83</td><td>█▇▆▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.85</td><td>█▇▆▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.86</td><td>█▇▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.88</td><td>██▇▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.89</td><td>██▇▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.90</td><td>██▇▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.91</td><td>██▇▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.92</td><td>██▇▆▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.93</td><td>█▇▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.94</td><td>███▇▇▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train/0.95</td><td>██▆▅▂▂▂▁▁▁▁▁▁██▆▅▅▅▃▂▁▁▁▁▁██▆▆▆▅▅▄▂▂▂▂▂▂</td></tr><tr><td>loss/train/0.96</td><td>███▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>prune_iter</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>sparsity</td><td>▁▁▂▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>summary/loss/evalbest</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃</td></tr><tr><td>summary/loss/evallast</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃</td></tr><tr><td>summary/loss/trainbest</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▅▅▅▅▆▆██</td></tr><tr><td>summary/loss/trainlast</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▅▅▅▅▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1500</td></tr><tr><td>loss/eval/0.00</td><td>0.12753</td></tr><tr><td>loss/eval/0.10</td><td>0.11442</td></tr><tr><td>loss/eval/0.20</td><td>0.1421</td></tr><tr><td>loss/eval/0.28</td><td>0.1246</td></tr><tr><td>loss/eval/0.36</td><td>0.0933</td></tr><tr><td>loss/eval/0.42</td><td>0.1017</td></tr><tr><td>loss/eval/0.49</td><td>0.09867</td></tr><tr><td>loss/eval/0.54</td><td>0.07098</td></tr><tr><td>loss/eval/0.59</td><td>0.07358</td></tr><tr><td>loss/eval/0.63</td><td>0.07282</td></tr><tr><td>loss/eval/0.67</td><td>0.06516</td></tr><tr><td>loss/eval/0.71</td><td>0.05621</td></tr><tr><td>loss/eval/0.74</td><td>0.05178</td></tr><tr><td>loss/eval/0.77</td><td>0.07253</td></tr><tr><td>loss/eval/0.79</td><td>0.07271</td></tr><tr><td>loss/eval/0.82</td><td>0.06611</td></tr><tr><td>loss/eval/0.83</td><td>0.12789</td></tr><tr><td>loss/eval/0.85</td><td>0.12478</td></tr><tr><td>loss/eval/0.86</td><td>0.14239</td></tr><tr><td>loss/eval/0.88</td><td>0.13545</td></tr><tr><td>loss/eval/0.89</td><td>0.18206</td></tr><tr><td>loss/eval/0.90</td><td>0.18332</td></tr><tr><td>loss/eval/0.91</td><td>0.18339</td></tr><tr><td>loss/eval/0.92</td><td>0.32002</td></tr><tr><td>loss/eval/0.93</td><td>0.31386</td></tr><tr><td>loss/eval/0.94</td><td>0.32353</td></tr><tr><td>loss/eval/0.95</td><td>0.52615</td></tr><tr><td>loss/eval/0.96</td><td>0.53421</td></tr><tr><td>loss/train/0.00</td><td>0.00328</td></tr><tr><td>loss/train/0.10</td><td>0.0022</td></tr><tr><td>loss/train/0.20</td><td>0.0023</td></tr><tr><td>loss/train/0.28</td><td>0.00428</td></tr><tr><td>loss/train/0.36</td><td>0.00356</td></tr><tr><td>loss/train/0.42</td><td>0.0047</td></tr><tr><td>loss/train/0.49</td><td>0.00544</td></tr><tr><td>loss/train/0.54</td><td>0.00422</td></tr><tr><td>loss/train/0.59</td><td>0.00488</td></tr><tr><td>loss/train/0.63</td><td>0.00561</td></tr><tr><td>loss/train/0.67</td><td>0.00526</td></tr><tr><td>loss/train/0.71</td><td>0.00589</td></tr><tr><td>loss/train/0.74</td><td>0.00511</td></tr><tr><td>loss/train/0.77</td><td>0.0061</td></tr><tr><td>loss/train/0.79</td><td>0.00611</td></tr><tr><td>loss/train/0.82</td><td>0.00669</td></tr><tr><td>loss/train/0.83</td><td>0.02473</td></tr><tr><td>loss/train/0.85</td><td>0.02424</td></tr><tr><td>loss/train/0.86</td><td>0.02774</td></tr><tr><td>loss/train/0.88</td><td>0.02826</td></tr><tr><td>loss/train/0.89</td><td>0.0758</td></tr><tr><td>loss/train/0.90</td><td>0.07587</td></tr><tr><td>loss/train/0.91</td><td>0.07592</td></tr><tr><td>loss/train/0.92</td><td>0.15663</td></tr><tr><td>loss/train/0.93</td><td>0.15801</td></tr><tr><td>loss/train/0.94</td><td>0.16153</td></tr><tr><td>loss/train/0.95</td><td>0.24408</td></tr><tr><td>loss/train/0.96</td><td>0.2506</td></tr><tr><td>prune_iter</td><td>31</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">IMP-reinit-with nograd</strong> at: <a href='https://wandb.ai/mxmn/init-thesis/runs/zq8ksc12' target=\"_blank\">https://wandb.ai/mxmn/init-thesis/runs/zq8ksc12</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231006_131410-zq8ksc12/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    device = get_pytorch_device()\n",
    "    device = 'cpu'\n",
    "    config = Config(\n",
    "        experiment=f'IMP-reinit-with nograd',\n",
    "        dataset=DATASET_NAME,\n",
    "        model_shape=[INPUT_DIM, 20, 20, OUTPUT_DIM],\n",
    "        model_class = SimpleMLP,\n",
    "\n",
    "        # pruning\n",
    "        pruning_levels=30,\n",
    "        pruning_rate=0.1,\n",
    "        pruning_strategy='global',\n",
    "        prune_weights=True,\n",
    "        prune_biases=False,\n",
    "\n",
    "        # training\n",
    "        training_epochs=1500,\n",
    "        lr=0.001,\n",
    "        momentum=0,\n",
    "        optimizer=ADAM,\n",
    "        batch_size = None,\n",
    "\n",
    "        # seeds\n",
    "        model_seed=3,\n",
    "        data_seed=2,\n",
    "\n",
    "        # lottery\n",
    "        reinit=True,\n",
    "\n",
    "        # storage\n",
    "        persist=True,\n",
    "        timestamp=datetime.now().strftime(\"%Y_%m_%d_%H%M%S\"),\n",
    "        device=str(device),\n",
    "        wandb=True, # does this even make any sense\n",
    "    )\n",
    "\n",
    "    # create the model, optimizer and dataloaders\n",
    "    model, loss_fn = build_model(config)\n",
    "    optim = build_optimizer(model, config)\n",
    "    train_loader, test_loader = build_loaders(config.batch_size)\n",
    "\n",
    "    save_hparams(config)\n",
    "\n",
    "    # run the experiment\n",
    "    with wandb.init(project=PROJECT, name=config.experiment, config=config):\n",
    "\n",
    "        model = imp.run(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            optim=optim,\n",
    "            loss_fn=loss_fn,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
