{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "description =(\n",
    "'''Take a model with very small size (4,8,8,2). It will function as the *base model*. Only considering weights for pruning, this model has 112 p-params.\n",
    "With a fixed p-rate of 0.32, a larger model architecture can be calculated, which would have the same number of p-parameters after pruning with (roughly) the p-rate once.\n",
    "This process is termed \"model extension\". A model is extended by n, where n is the number of p-levels needed to get it to the size of the base model. n is termed the \"extension-level\".\n",
    "The model architectures are restricted to having the same number of hidden layers, and the same number of hidden neurons in each hidden layer.\n",
    "Therefore, the exact p-rate differs slightly, depending on the extension level.\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_ids=['wpcowdl5', 'j9lvvxjg', 'a5kr3muw']\n",
    "\n",
    "try: print('Histories already loaded.') if hc else None\n",
    "except: hc = load_histories_and_configs(sweep_ids)\n",
    "\n",
    "# define dataframes to concat\n",
    "_split = make_split_df(hc)\n",
    "_levels = make_df(hc, 'extension_levels')\n",
    "_shape = make_df(hc, 'model_shape').map(lambda shape: shape[1])\n",
    "_p_params_begin = make_df(hc, 'param_trajectory').map(lambda x: x[0])\n",
    "#_shape['hidden_dim'] = _shape.map(lambda shape: shape[1])\n",
    "_splitrange = make_splitrange_df(hc)\n",
    "_first, _last = make_dfs_at_splitrange(hc, _splitrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_active_w = make_df_from_history(hc, 'active-weights-rel', invert=True, drop_first=True)\n",
    "_loss = make_df_from_history(hc, 'val-loss', invert=True, drop_first=True)\n",
    "\n",
    "df_loss = pd.concat([_levels, _loss], axis=1).groupby('extension_levels').mean()\n",
    "df_active_weights = pd.concat([_levels, _active_w], axis=1).groupby('extension_levels').mean()\n",
    "\n",
    "_active_w_abs = make_df_from_history(hc, 'active-weights-abs', invert=True, drop_first=True)\n",
    "df_active_weights_abs = pd.concat([_levels, _active_w_abs], axis=1).groupby('extension_levels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis \n",
    "## 'Collateral Damage':\n",
    "for each p-level, there is 'collateral damage'. Some p-parameters are made inactive or zombious. -> the relative amount of active p-parameters decreases, the more p-levels there are.\n",
    "### How to test\n",
    "plot the relative amount of at each level, from each run. If it is true, each run with more levels should have less active p-parameters at the corresponding extension level. Because the networks have the same number of p-parameters at the corresponding levels the comparisson should be fair.\n",
    "\n",
    "Indeed, the more pruning iterations, generally decrease the percentage of active-weights in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 6, 12, 18, 24]\n",
    "plt.plot(df_active_weights.iloc[indices].T, marker='o')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Performance\n",
    "The networks seem to reach peak performance around the same number of p-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_active_weights_abs.index\n",
    "\n",
    "for i in indices:\n",
    "    x = df_active_weights_abs.loc[i]\n",
    "    y = df_loss.loc[i]\n",
    "    plt.plot(x,y, marker='', label=f'e={i}', alpha=0.4)\n",
    "    \n",
    "plt.xscale('log'), plt.yscale('log')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do networks split\n",
    "and when do they degrade?\n",
    "Why do networks only start to degrade after 12 extension levels, and continue to do so when extending further?\n",
    "Lets look at the number of active parameters at the time they split and at the time they degrade.\n",
    "\n",
    "Do they split earlier, in terms of iteration and in terms of active weights?\n",
    "\n",
    "Results show some signs that tey split in earlier iterations, but not clearly. Could be noise as well.\n",
    "However, the more e-levels they have, the less active parameters the have when they split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = pd.concat([_levels, _last['active-weights-abs']],axis=1).groupby('extension_levels').mean()\n",
    "active_std = pd.concat([_levels, _last['active-weights-abs']],axis=1).groupby('extension_levels').agg(np.std, ddof=0)\n",
    "\n",
    "\n",
    "pparams = pd.concat([_levels, _last['pparams']],axis=1).groupby('extension_levels').mean()\n",
    "\n",
    "plt.errorbar(x =active.index, y=active, yerr=active_std.values.squeeze(), label='active', marker='o', alpha=0.6)\n",
    "plt.plot(pparams, label='available', marker='o', alpha=0.6)\n",
    "plt.xticks(active.index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do they degrade earlier?\n",
    "They clearly degrade with more p-parameters available, meaning they degrade in earlier iterations. However, the number of active parameters stays about the same, independent of the number of e-levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = pd.concat([_levels, _first['active-weights-abs']],axis=1).groupby('extension_levels').mean()\n",
    "plt.plot(active, label='active', marker='o', alpha=0.6)\n",
    "\n",
    "#active_features = pd.concat([_levels, _first['active-features-abs']],axis=1).groupby('extension_levels').mean()\n",
    "#plt.plot(active_features, label='features', marker='o', alpha=0.6)\n",
    "\n",
    "pparams = pd.concat([_levels, _first['pparams']],axis=1).groupby('extension_levels').mean()\n",
    "plt.plot(pparams, label='available', marker='o', alpha=0.6)\n",
    "\n",
    "plt.xticks(active.index)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when Looking at weights and biases together, the Split networks actually are larger when increasing the e-level. this is mainly due to more biases in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz = pd.concat([_levels, _first['active-abs']],axis=1).groupby('extension_levels').mean()\n",
    "plt.plot(fz, label='active parameters', marker='o', alpha=0.6)\n",
    "\n",
    "lz = pd.concat([_levels, _last['active-abs']],axis=1).groupby('extension_levels').mean()\n",
    "plt.plot(lz, label='active parameters', marker='o', alpha=0.6)\n",
    "\n",
    "plt.xticks(active.index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can this be true?\n",
    "That there are 7 times the number of biases than weights ACTIVE in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_first['active-features-abs'] / _first['active-weights-abs']\n",
    "fz = pd.concat([_levels, _first['active-features-abs'] / _first['active-weights-abs']],axis=1).groupby('extension_levels').mean()\n",
    "lz = pd.concat([_levels, _last['active-features-abs'] / _last['active-weights-abs']],axis=1).groupby('extension_levels').mean()\n",
    "plt.plot(lz, label='last weights / biases ratio', marker='o', alpha=0.6)\n",
    "plt.plot(fz, label='first weights / biases ratio', marker='o', alpha=0.6)\n",
    "\n",
    "plt.xticks(active.index)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfirst = pd.concat([_levels, _last['active-weights-rel']],axis=1).groupby('extension_levels').mean()\n",
    "ylast = pd.concat([_levels, _first['active-weights-rel']],axis=1).groupby('extension_levels').mean()\n",
    "\n",
    "plt.plot(yfirst, label='first', marker='o', alpha=0.6)\n",
    "plt.plot(ylast, label='last', marker='o', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "# Plotting a heatmap with a logarithmic colorbar using matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(data, cmap='tab20', aspect='auto', norm=LogNorm())\n",
    "plt.colorbar(label='Logarithmic Value')\n",
    "plt.xticks(ticks=range(data.shape[1]), labels=data.columns)\n",
    "plt.yticks(ticks=range(data.shape[0]), labels=data.index)\n",
    "plt.title(\"Heatmap of DataFrame (Logarithmic Scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Collateral Damage'\n",
    "\n",
    "w_active = pd.DataFrame([h['active-weights-abs'] for h,_ in hc]).reset_index().drop(0, axis=1)\n",
    "df = pd.concat([_levels,w_active], axis=1).sort_values('extension_levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='Extending the base model'\n",
    "\n",
    "ydf = pd.concat([_split, _levels], axis=1).groupby('extension_levels').sum()\n",
    "xdf = pd.concat([_shape, _levels], axis=1).drop_duplicates()\n",
    "xdf['xticklabels'] = xdf.apply(lambda df: f\"{df['extension_levels']}\\n{df['model_shape']}\", axis=1)\n",
    "xdf = xdf.sort_values('extension_levels').reset_index(drop=True)\n",
    "xdf['xticks'] = xdf.index\n",
    "\n",
    "\n",
    "xlabel='e-levels\\nhidden-dim'\n",
    "description=(\n",
    "'''\n",
    "'''\n",
    ")\n",
    "fig, axes = plt.subplots(nrows=len(ydf.columns), figsize=(16,9), sharey=True, sharex=False)\n",
    "fig.suptitle(title, fontweight='regular', fontsize=15, fontname='Helvetica')\n",
    "fig.text(0.5, 0.95, description, ha='center', va='top', fontsize=9, color=(0.1,0.1,0.1))\n",
    "\n",
    "# plot the data\n",
    "ydf.plot.bar(\n",
    "    ax=axes,\n",
    "    subplots=True,\n",
    "    width=0.95,  \n",
    "    alpha=0.8,\n",
    "    colormap='viridis',\n",
    "    rot=0,\n",
    ")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yticks(np.arange(np.max(ydf)+1))\n",
    "    ax.set_xticks(xdf['xticks'])\n",
    "    ax.set_xticklabels(xdf['xticklabels'], fontsize=9, color='gray')\n",
    "    ax.set_ylabel('num models', fontsize=9)\n",
    "    ax.set_xlabel(xlabel, fontsize=9, color='gray')\n",
    "    ax.xaxis.set_label_coords(0.0, -0.087)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "set_style(axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
